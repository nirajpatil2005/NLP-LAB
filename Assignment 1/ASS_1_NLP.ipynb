{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htHO6I__Sovi",
        "outputId": "24bad158-d671-4598-901d-d5d4b4ad0c53"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Australia fast bowler Pat Cummins was ruled out on Saturday (January 31, 2026) for next month's ‍Twenty20 World Cup after failing to recover from a nagging ​back injury, and Ben Dwarshuis will replace him ‌for the global showpiece.\""
      ],
      "metadata": {
        "id": "BwOwXpHUSpRg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Based on Spaces\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "wt = WhitespaceTokenizer()\n",
        "print(\"Whitespace Tokenization:\")\n",
        "print(wt.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPwd4QXlTEPB",
        "outputId": "557dcf23-34f3-40d3-c38b-e6caa0db5b16"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization:\n",
            "['Australia', 'fast', 'bowler', 'Pat', 'Cummins', 'was', 'ruled', 'out', 'on', 'Saturday', '(January', '31,', '2026)', 'for', 'next', \"month's\", '\\u200dTwenty20', 'World', 'Cup', 'after', 'failing', 'to', 'recover', 'from', 'a', 'nagging', '\\u200bback', 'injury,', 'and', 'Ben', 'Dwarshuis', 'will', 'replace', 'him', '\\u200cfor', 'the', 'global', 'showpiece.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Words and Punctuation\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "wpt = WordPunctTokenizer()\n",
        "print(\"\\nPunctuation-based Tokenization:\")\n",
        "print(wpt.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R_CytC7TGYO",
        "outputId": "1a945932-c6a6-4cdb-ff56-5c3cbeb21c0b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Punctuation-based Tokenization:\n",
            "['Australia', 'fast', 'bowler', 'Pat', 'Cummins', 'was', 'ruled', 'out', 'on', 'Saturday', '(', 'January', '31', ',', '2026', ')', 'for', 'next', 'month', \"'\", 's', '\\u200d', 'Twenty20', 'World', 'Cup', 'after', 'failing', 'to', 'recover', 'from', 'a', 'nagging', '\\u200b', 'back', 'injury', ',', 'and', 'Ben', 'Dwarshuis', 'will', 'replace', 'him', '\\u200c', 'for', 'the', 'global', 'showpiece', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Grammer Aware Tokenization\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tbt = TreebankWordTokenizer()\n",
        "print(\"\\nTreebank Tokenization:\")\n",
        "print(tbt.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlCk2ra_TK5y",
        "outputId": "9f1d2acf-b8c0-4722-a4a0-1486ad8431fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treebank Tokenization:\n",
            "['Australia', 'fast', 'bowler', 'Pat', 'Cummins', 'was', 'ruled', 'out', 'on', 'Saturday', '(', 'January', '31', ',', '2026', ')', 'for', 'next', 'month', \"'s\", '\\u200dTwenty20', 'World', 'Cup', 'after', 'failing', 'to', 'recover', 'from', 'a', 'nagging', '\\u200bback', 'injury', ',', 'and', 'Ben', 'Dwarshuis', 'will', 'replace', 'him', '\\u200cfor', 'the', 'global', 'showpiece', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "print(\"\\nTweet Tokenization:\")\n",
        "print(tt.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUCiQahaTNm3",
        "outputId": "ce5ed0b0-66fb-490c-9621-bc69fd95a999"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenization:\n",
            "['Australia', 'fast', 'bowler', 'Pat', 'Cummins', 'was', 'ruled', 'out', 'on', 'Saturday', '(', 'January', '31', ',', '2026', ')', 'for', 'next', \"month's\", ' \\u200dT', 'wenty', '20', 'World', 'Cup', 'after', 'failing', 'to', 'recover', 'from', 'a', 'nagging', '\\u200b', 'back', 'injury', ',', 'and', 'Ben', 'Dwarshuis', 'will', 'replace', 'him', '\\u200cfor', 'the', 'global', 'showpiece', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine two words as single token\n",
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe = MWETokenizer([('machine', 'learning'), ('artificial', 'intelligence')])\n",
        "sentence = \"I love machine learning and artificial intelligence\"\n",
        "print(\"\\nMWE Tokenization:\")\n",
        "print(mwe.tokenize(sentence.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atDS-hD8TV9-",
        "outputId": "7ef1d845-92bb-4561-f7d8-c6a73106dbb1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MWE Tokenization:\n",
            "['I', 'love', 'machine_learning', 'and', 'artificial_intelligence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "words = [\"running\", \"flies\", \"easily\", \"fairness\",\"Properly\"]\n",
        "\n",
        "print(\"\\nPorter Stemming:\")\n",
        "for w in words:\n",
        "    print(w, \"→\", ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhqtH2tsTbP2",
        "outputId": "2b7abe39-4d05-4dc6-de4c-cd15a968e316"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Porter Stemming:\n",
            "running → run\n",
            "flies → fli\n",
            "easily → easili\n",
            "fairness → fair\n",
            "Properly → properli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "print(\"\\nSnowball Stemming:\")\n",
        "for w in words:\n",
        "    print(w, \"→\", ss.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEJeSDFfTmUZ",
        "outputId": "75d941fb-7b6e-4e23-92ab-6ee1bac83260"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Snowball Stemming:\n",
            "running → run\n",
            "flies → fli\n",
            "easily → easili\n",
            "fairness → fair\n",
            "Properly → proper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"better\", \"cars\", \"flies\"]\n",
        "\n",
        "print(\"\\nLemmatization:\")\n",
        "for w in words:\n",
        "    print(w, \"→\", lemmatizer.lemmatize(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vsRwNXBTvNV",
        "outputId": "1db5526a-4081-4b3a-b1fe-b10e42fc7dc1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization:\n",
            "running → running\n",
            "better → better\n",
            "cars → car\n",
            "flies → fly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTUS0UYZTydZ",
        "outputId": "7ffce8b2-e2de-40db-f1d0-8c2786992dbf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"better\", \"cars\", \"flies\"]\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    if tag == 'J':\n",
        "        return wordnet.ADJ\n",
        "    elif tag == 'V':\n",
        "        return wordnet.VERB\n",
        "    elif tag == 'N':\n",
        "        return wordnet.NOUN\n",
        "    elif tag == 'R':\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "print(\"Lemmatization:\")\n",
        "for w in words:\n",
        "    pos = get_wordnet_pos(w)\n",
        "    print(w, \"→\", lemmatizer.lemmatize(w, pos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii_lh7D_T3SG",
        "outputId": "62f16d02-7e93-43b2-d032-7e25077a02fd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization:\n",
            "running → run\n",
            "better → well\n",
            "cars → car\n",
            "flies → fly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MaZCI2ZAT7KY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}