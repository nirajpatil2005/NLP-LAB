{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w76jHymCVsPo"
      },
      "outputs": [],
      "source": [
        "# Sample documents (sentences)\n",
        "sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"Hi there!\",\n",
        "    \"Good morning\",\n",
        "    \"Can you help me with my order?\",\n",
        "    \"I need support for my account\",\n",
        "    \"Please reset my password\",\n",
        "    \"This app keeps crashing\",\n",
        "    \"I am unhappy with the service\",\n",
        "    \"The delivery was very late\",\n",
        "    \"Hey!\",\n",
        "    \"Could you tell me the price?\",\n",
        "    \"Why is this so slow?\"\n",
        "]\n",
        "\n",
        "# Corresponding labels (string format)\n",
        "labels = [\n",
        "    \"greeting\",   # Hello, how are you?\n",
        "    \"greeting\",   # Hi there!\n",
        "    \"greeting\",   # Good morning\n",
        "    \"request\",    # Can you help me with my order?\n",
        "    \"request\",    # I need support for my account\n",
        "    \"request\",    # Please reset my password\n",
        "    \"complaint\",  # This app keeps crashing\n",
        "    \"complaint\",  # I am unhappy with the service\n",
        "    \"complaint\",  # The delivery was very late\n",
        "    \"greeting\",   # Hey!\n",
        "    \"request\",    # Could you tell me the price?\n",
        "    \"complaint\"   # Why is this so slow?\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "cleaned_docs = [clean_text(doc) for doc in sentences]\n",
        "print(\"Cleaned Text:\")\n",
        "print(cleaned_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re_OVDhCWR2b",
        "outputId": "6beac60b-b797-4b16-d180-4f9c73573645"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text:\n",
            "['hello how are you', 'hi there', 'good morning', 'can you help me with my order', 'i need support for my account', 'please reset my password', 'this app keeps crashing', 'i am unhappy with the service', 'the delivery was very late', 'hey', 'could you tell me the price', 'why is this so slow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_docs = []\n",
        "for doc in cleaned_docs:\n",
        "    tokens = word_tokenize(doc)\n",
        "    lemmas = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    lemmatized_docs.append(\" \".join(lemmas))\n",
        "\n",
        "print(\"\\nLemmatized Text:\")\n",
        "print(lemmatized_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGhKPK3WWd0x",
        "outputId": "abab8202-c083-4d3a-8974-3ed2c90b2fca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized Text:\n",
            "['hello how are you', 'hi there', 'good morning', 'can you help me with my order', 'i need support for my account', 'please reset my password', 'this app keep crashing', 'i am unhappy with the service', 'the delivery wa very late', 'hey', 'could you tell me the price', 'why is this so slow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "final_docs = []\n",
        "for doc in lemmatized_docs:\n",
        "    tokens = doc.split()\n",
        "    filtered = [word for word in tokens if word not in stop_words]\n",
        "    final_docs.append(\" \".join(filtered))\n",
        "\n",
        "print(\"\\nAfter Stop-word Removal:\")\n",
        "print(final_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsWFAEVkWg0c",
        "outputId": "07215ca7-0229-498a-9d27-54ea5075aa85"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Stop-word Removal:\n",
            "['hello', 'hi', 'good morning', 'help order', 'need support account', 'please reset password', 'app keep crashing', 'unhappy service', 'delivery wa late', 'hey', 'could tell price', 'slow']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "\n",
        "print(\"\\nEncoded Labels:\")\n",
        "print(encoded_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i4XJiGLWjTr",
        "outputId": "c3b31f4d-9f49-4ebb-d2b0-f465b6657c20"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoded Labels:\n",
            "[1 1 1 2 2 2 0 0 0 1 2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(final_docs)\n",
        "\n",
        "print(\"\\nTF-IDF Vocabulary:\")\n",
        "print(tfidf.get_feature_names_out())\n",
        "\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aRdBP8nWlZ0",
        "outputId": "107b9584-9196-4b47-9698-025e12237c11"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Vocabulary:\n",
            "['account' 'app' 'could' 'crashing' 'delivery' 'good' 'hello' 'help' 'hey'\n",
            " 'hi' 'keep' 'late' 'morning' 'need' 'order' 'password' 'please' 'price'\n",
            " 'reset' 'service' 'slow' 'support' 'tell' 'unhappy' 'wa']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         1.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.70710678\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.70710678 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.70710678 0.         0.         0.         0.\n",
            "  0.         0.         0.70710678 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.57735027 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.57735027 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.57735027 0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.57735027 0.57735027 0.\n",
            "  0.57735027 0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.57735027 0.         0.57735027 0.         0.\n",
            "  0.         0.         0.         0.         0.57735027 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.70710678 0.         0.         0.         0.70710678\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.57735027 0.\n",
            "  0.         0.         0.         0.         0.         0.57735027\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.57735027]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.57735027 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.57735027\n",
            "  0.         0.         0.         0.         0.57735027 0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.         0.         0.\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Save cleaned text\n",
        "df_text = pd.DataFrame({\n",
        "    \"original_text\": sentences,\n",
        "    \"processed_text\": final_docs,\n",
        "    \"label\": encoded_labels\n",
        "})\n",
        "df_text.to_csv(\"processed_text.csv\", index=False)\n",
        "\n",
        "# Save TF-IDF matrix\n",
        "df_tfidf = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf.get_feature_names_out()\n",
        ")\n",
        "df_tfidf.to_csv(\"tfidf_vectors.csv\", index=False)\n",
        "\n",
        "print(\"\\nFiles saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtD4hnLzWnww",
        "outputId": "9c058c5a-a55c-4b1a-aa26-c2336db13ceb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Files saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwOeeXenWp78"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}